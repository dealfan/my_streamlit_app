import streamlit as st
import pandas as pd
import joblib
import pickle
import jieba
import re
import io

# è‡ªå®šä¹‰ CSS æ ·å¼
st.markdown(
    """
    <style>
        /* é¡µé¢èƒŒæ™¯ */
        body {
            background-color: #f9f9f9;
        }
        /* å•é€‰æŒ‰é’®ç¾åŒ– */
        .stRadio input[type="radio"] {
            accent-color: #FF5733; /* æ”¹å˜é€‰ä¸­æ—¶çš„é¢œè‰² */
        }
        .stRadio label {
            font-size: 16px; /* è°ƒæ•´å­—ä½“å¤§å° */
            color: #333; /* è°ƒæ•´æ–‡å­—é¢œè‰² */
        }
        .stRadio input[type="radio"]:checked + label {
            color: #FF5733; /* é€‰ä¸­æ—¶çš„æ–‡å­—é¢œè‰² */
        }
        /* è¾“å…¥æ¡†ç¾åŒ– */
        .stTextInput textarea {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            font-size: 14px;
        }
        /* æ–‡ä»¶ä¸Šä¼ å™¨ç¾åŒ– */
        .stFileUploader div > div > input {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            font-size: 14px;
        }
        /* æ•°æ®è¡¨æ ¼ç¾åŒ– */
        .stDataFrame table {
            font-size: 14px;
            border-collapse: collapse;
            width: 100%;
        }
        .stDataFrame th, .stDataFrame td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .stDataFrame th {
            background-color: #f2f2f2;
        }
    </style>
    """,
    unsafe_allow_html=True,
)

TFIDF_PICKLE = 'tfidf_vectorizer.pkl'
MODEL_PICKLE = 'ensemble_news_model.pkl'
STOPWORDS_FILE = 'cnews.vocab.txt'

@st.cache_resource  # ä½¿ç”¨ st.cache_resource ç¼“å­˜èµ„æºå¯¹è±¡
def load_vectorizer():
    with open(TFIDF_PICKLE, 'rb') as f:
        return pickle.load(f)

@st.cache_resource  # ä½¿ç”¨ st.cache_resource ç¼“å­˜èµ„æºå¯¹è±¡
def load_model():
    return joblib.load(MODEL_PICKLE)

@st.cache_data  # ä½¿ç”¨ st.cache_data ç¼“å­˜åœç”¨è¯é›†åˆ
def load_stopwords():
    stopwords = set()
    try:
        with open(STOPWORDS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                word = line.strip()
                if word:
                    stopwords.add(word)
    except Exception as e:
        st.error("åœç”¨è¯åŠ è½½é”™è¯¯: " + str(e))
    return stopwords

def clean_text(text):
    text = re.sub(r'[^\u4e00-\u9fa5]', ' ', str(text))  # ä¿ç•™ä¸­æ–‡å­—ç¬¦
    return text.strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦

def tokenize_text(text, stopwords):
    text = clean_text(text)
    if not text:  # å¦‚æœæ¸…ç†åä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™è¿”å›ç©º
        return ""
    tokens = jieba.lcut(text)
    tokens = [tok for tok in tokens if tok.strip() and tok not in stopwords]
    return " ".join(tokens)

def classify_texts(texts, stopwords, vectorizer, model):
    processed = []
    non_empty_indices = []  # è®°å½•éç©ºè¡Œçš„ç´¢å¼•
    for i, text in enumerate(texts):
        cleaned_text = clean_text(text)
        if cleaned_text:  # è·³è¿‡ç©ºç™½è¡Œ
            processed.append(tokenize_text(cleaned_text, stopwords))
            non_empty_indices.append(i)

    if not processed:  # å¦‚æœæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºç™½è¡Œ
        return []

    X_input = vectorizer.transform(processed)
    preds = model.predict(X_input)

    # å°†é¢„æµ‹ç»“æœæ˜ å°„å›åŸå§‹æ–‡æœ¬ä½ç½®ï¼ˆåŒ…æ‹¬ç©ºç™½è¡Œï¼‰
    full_preds = [""] * len(texts)  # åˆå§‹åŒ–å…¨ä¸ºç©ºå­—ç¬¦ä¸²
    for idx, pred in zip(non_empty_indices, preds):
        full_preds[idx] = pred

    return full_preds

def main():
    st.title("âœ¨ ä¸­æ–‡æ–°é—»åˆ†ç±»ç³»ç»Ÿ âœ¨")
    st.markdown("""
    æ‚¨å¯ä»¥é€‰æ‹©è¾“å…¥ä¸€æ®µæˆ–å¤šæ®µæ–‡æœ¬ï¼Œæˆ–ä¸Šä¼ åŒ…å«æ–°é—»æ–‡æœ¬çš„ `.txt` æˆ– `.csv` æ–‡ä»¶ï¼Œ  
    ç³»ç»Ÿå°†ä¸ºæ‚¨åˆ†ç±»æ¯æ¡æ–‡æœ¬ï¼Œå¹¶æ˜¾ç¤ºç»“æœã€‚
    """)

    # ç”¨æˆ·é€‰æ‹©è¾“å…¥æ–¹å¼
    input_mode = st.radio(
        "è¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š",
        ["è¾“å…¥æ–‡æœ¬", "ä¸Šä¼ æ–‡ä»¶"],
        key="input_mode_radio",
        horizontal=True  # æ°´å¹³æ’åˆ—å•é€‰æŒ‰é’®
    )

    stopwords = load_stopwords()
    vectorizer = load_vectorizer()
    model = load_model()

    if input_mode == "è¾“å…¥æ–‡æœ¬":
        # å¤šæ®µæ–‡æœ¬è¾“å…¥
        # st.markdown("è¯·è¾“å…¥å¤šæ®µæ–°é—»æ–‡æœ¬ï¼Œæ¯æ®µæ–‡æœ¬ç”¨æ¢è¡Œç¬¦åˆ†éš”ï¼š")
        text_input = st.text_area(
            "",
            placeholder="ä¾‹å¦‚ï¼š\nä»Šå¤©çš„å¤©æ°”éå¸¸å¥½\næ˜¨å¤©è‚¡å¸‚å¤§è·Œ",
            height=150
        )
        if st.button("å¼€å§‹åˆ†ç±»"):
            if text_input.strip():  # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
                # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²è¾“å…¥å†…å®¹
                texts = text_input.splitlines()
                predictions = classify_texts(texts, stopwords, vectorizer, model)
                
                # å±•ç¤ºç»“æœ
                result_df = pd.DataFrame({
                    "è¾“å…¥æ–‡æœ¬": texts,
                    "åˆ†ç±»ç»“æœ": predictions
                })
                st.success("ğŸ‰ åˆ†ç±»å®Œæˆï¼ä»¥ä¸‹æ˜¯ç»“æœï¼š")
                st.dataframe(result_df, use_container_width=True)
            else:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹ï¼")

    elif input_mode == "ä¸Šä¼ æ–‡ä»¶":
        # æ–‡ä»¶ä¸Šä¼ 
        file = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶ä¸Šä¼ ",
            type=['txt', 'csv'],
            accept_multiple_files=False
        )

        if file is not None:
            try:
                # åˆå§‹åŒ– dfï¼Œé»˜è®¤ä¸ºç©º DataFrame
                df = pd.DataFrame()

                # è‡ªåŠ¨åˆ¤æ–­æ–‡ä»¶æ ¼å¼
                if file.name.endswith('.csv'):
                    df = pd.read_csv(file)
                    st.write("æ£€æµ‹åˆ° CSV æ–‡ä»¶")
                    # å°è¯•è¯†åˆ«æ–‡æœ¬åˆ—
                    if df.empty:
                        st.error("CSV æ–‡ä»¶ä¸ºç©ºï¼Œè¯·ä¸Šä¼ æœ‰æ•ˆæ–‡ä»¶ï¼")
                        return
                    text_col = st.selectbox("è¯·é€‰æ‹©æ–‡æœ¬åˆ—ï¼š", df.columns)
                    texts = df[text_col].fillna("").tolist()  # æ›¿æ¢ NaN ä¸º ""
                else:
                    st.write("æ£€æµ‹åˆ° TXT æ–‡ä»¶ï¼Œæ¯è¡Œä¸ºä¸€æ¡æ–°é—»")
                    texts = file.read().decode('utf-8').splitlines()
                    df = pd.DataFrame({'text': texts})

                # ç¡®ä¿æœ‰æ•°æ®
                if df.empty or not texts:
                    st.error("æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¯·ä¸Šä¼ æœ‰æ•ˆæ–‡ä»¶ï¼")
                    return

                # æ‰¹é‡åˆ†ç±»
                predictions = classify_texts(texts, stopwords, vectorizer, model)
                df['predicted_label'] = predictions

                st.success("ğŸ‰ åˆ†ç±»å®Œæˆï¼ä»¥ä¸‹æ˜¯éƒ¨åˆ†ç»“æœï¼š")
                st.dataframe(df.head(10), use_container_width=True)

                # ä¸‹è½½é“¾æ¥
                output = io.StringIO()
                df.to_csv(output, index=False, encoding='utf-8')
                st.download_button(
                    label="ç‚¹å‡»ä¸‹è½½åˆ†ç±»ç»“æœCSV",
                    data=output.getvalue(),
                    file_name='classified_news.csv',
                    mime='text/csv'
                )

            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶å‡ºé”™ï¼š{e}")

if __name__ == '__main__':
    main()